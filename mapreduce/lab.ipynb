{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to this mapreduce with Spark lab !\n",
    "\n",
    "During this lab, we are going to explore some simple examples of how to\n",
    "write big data pipelines using spark, based on the principles of mapreduce.\n",
    "\n",
    "A very important resource to look for help is [the spark documentation](https://spark.apache.org/docs/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName(\"MySparkApp\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '5G')\n",
    "        .set('spark.driver.memory', '6G')\n",
    "        .set('spark.driver.maxResultSize', '4G'))\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Compute Pi\n",
    "\n",
    "We are going to use a Monte-Carlo estimation of the number $\\pi$.\n",
    "For more information about this method, [see here.](https://academo.org/demos/estimating-pi-monte-carlo/)\n",
    "\n",
    "To start, implements two ways to compute Pi with the monte-carlo method, with and without Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularPi(n: int) -> float:\n",
    "    \"\"\"Estimates Pi using just python.\"\"\"\n",
    "    pass\n",
    "\n",
    "def sparkPi(n: int) -> float:\n",
    "    \"\"\"Estimates Pi using spark\"\"\"\n",
    "    pass\n",
    "\n",
    "num_samples = 1000000\n",
    "print(\"Estimation of Pi: {0}\".format(sparkPi(num_samples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To understand the power of parallel computing (provided your computer has several cores), let's measure the execution time over a big number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronometer(func, n: int) -> float:\n",
    "    \"\"\"Measure the execution time of the function func with the parameter n\"\"\"\n",
    "    before = time.time()\n",
    "    func(n)\n",
    "    after = time.time()\n",
    "    return after - before\n",
    "\n",
    "\n",
    "def gain(n: int):\n",
    "    \"\"\"Measure the speed factor of the spark function over the regular one, when run over n samples.\"\"\"\n",
    "    return chronometer(regularPi, n) / chronometer(sparkPi, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does the gain evolve with the number of n samples? Why? Can you plot it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: around word count\n",
    "\n",
    "We are going to use the raw file from the BBC data set that can be found at http://mlg.ucd.ie/datasets/bbc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: load the data\n",
    "\n",
    "write the function to load the data as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFilenames(folder) -> list:\n",
    "    \"\"\"returns the list of file names to where the bbc data is\"\"\"\n",
    "    pass\n",
    "\n",
    "def loadDataFrame(folder):\n",
    "    filenames = getFilenames(folder)\n",
    "    return spark.read.text(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second step: Exploration\n",
    "\n",
    "Let's explore what have been loaded.\n",
    "- Can you check the value of one entry? of several?\n",
    "- How many entries do you have?\n",
    "- How many non empty ones?\n",
    "- Each entry is a text, can you write a mapreduce to plot the distribution of sizes of thoses texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thrid step: the word count map reduce\n",
    "\n",
    "- Can you write a map reduce to count the number of occurrences of each word in the dataset\n",
    "- Are you satisfied with the output? Why?\n",
    "- How to clean the input?\n",
    "- How to save the data on disk ?\n",
    "- Can you write a version with a combiner? What is the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordCountMapper(text) -> list:\n",
    "    \"\"\"given a text, returns a list of tuples (term, count)\"\"\"\n",
    "    pass\n",
    "\n",
    "def wordCountReduce(x, y):\n",
    "    \"\"\"Given a two counts, returns the combined count.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Questions\n",
    "\n",
    "- We are counting only single words, can you count ngrams?\n",
    "- How to find which ngrams are statistically meaningful?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
